{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c92096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv(\"../data/small_dataset.txt\", header=None, names=[\"item\", \"user\", \"ranking\"]).iloc[:5000]\n",
    "#ds.iloc[:5000].to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bef5c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>user</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     user  ranking\n",
       "0     1  1488844        3\n",
       "1     1   822109        5\n",
       "2     1   885013        4\n",
       "3     1    30878        4\n",
       "4     1   823519        3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f494c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4829"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ds['user']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c1e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make necessary imports\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import sql\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fd0b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x):\n",
    "    s = sum(x[1])\n",
    "    s_last = 0\n",
    "    x_new = []\n",
    "    for i in range(len(x[1])-1):\n",
    "        x_new.append(x[1][i] / s)\n",
    "        s_last += x_new[i]\n",
    "    x_new.append(1 - s_last)\n",
    "    return (x[0], np.array(x_new))\n",
    "\n",
    "def create_uniform_rdd_rowindexed_matrix(sc, nrow, ncol, normalize=False):\n",
    "    rdd = []\n",
    "    for i in range(nrow):\n",
    "        rdd.append((i, np.random.rand(ncol)))\n",
    "    #print(rdd)\n",
    "    rdd = sc.parallelize(rdd)\n",
    "    if normalize:\n",
    "        rdd = rdd.map(normalization)\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef1eaf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_h(hxy_tuple):\n",
    "    hx, y = hxy_tuple\n",
    "    h, x = hx\n",
    "    d = h @ y # a scalar(D_ii)\n",
    "    e = h @ x.reshape(len(x[0]))\n",
    "    h_new = h * np.divide(x+d, y+e) # broadcase add\n",
    "    return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ddb1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_i_R(ratings, sum_ratings, item_probs, ld):\n",
    "    \"\"\"Implement the function to compute the probability of items i, given the relevance context of users u.\n",
    "    Gets as input information on ratings and total ratings for all users of a cluster, then implements multiple\n",
    "    loops to calculate required outputs.\"\"\"\n",
    "    \n",
    "    # From broadcast values\n",
    "    sum_ratings = dict(sum_ratings)\n",
    "    item_probs = dict(item_probs)\n",
    "    \n",
    "    # Dictionary with nexted dictionaries. Stores for each user,\n",
    "    # all the items and their p(i|u) item user probability\n",
    "    user_items = dict()\n",
    "    \n",
    "    for user, item, rating in ratings:\n",
    "        \n",
    "        # Find sum of ratings for user and items\n",
    "        total_ratings = sum_ratings[user]\n",
    "        total_items = item_probs[item]\n",
    "        \n",
    "        # Calculate probability p(i|u), and stores it in user_items nested dictionary\n",
    "        prob = ((1 - ld) * rating / total_ratings) + ld * total_items\n",
    "            \n",
    "        if user in user_items:\n",
    "            user_items[user][item] = prob\n",
    "        else:\n",
    "            user_items[user] = dict()\n",
    "            user_items[user][item] = prob\n",
    "    \n",
    "    # Find the set of all user item pairs\n",
    "    user_item_pairs = set()\n",
    "    for user in user_items.keys():\n",
    "        for item in item_probs.keys():\n",
    "            user_item_pairs.add((user, item))\n",
    "    \n",
    "    out = dict()\n",
    "    # For every user item pair (i.e. every cell in prediction matrix A)\n",
    "    for user, item in user_item_pairs:\n",
    "        \n",
    "        prod = 1\n",
    "        # Loop over all items that have been assigned a probability by this user\n",
    "        for item_t in user_items[user].keys():\n",
    "            \n",
    "            sm = 0\n",
    "            # Loop over all users in the neighbourhood (cluster)\n",
    "            for user_j in user_items.keys():\n",
    "                \n",
    "                # Compute the formula for this particular (item, user)\n",
    "                p_item_j = user_items[user_j].get(item, 0)\n",
    "                p_t_j = user_items[user_j].get(item_t, 0)\n",
    "                \n",
    "                # Sum over the inner loop\n",
    "                sm += p_item_j * p_t_j\n",
    "        \n",
    "        # Take a product over the outer loop        \n",
    "        prod *= sm\n",
    "        \n",
    "        # Store the (item, user) probability\n",
    "        out[(item, user)] = prod\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "030fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRAlgorithm:\n",
    "    \n",
    "    def __init__(self, datapath, k, ld):\n",
    "        self.k = k\n",
    "        self.ld = ld\n",
    "        self.n = 2649429\n",
    "        self.m = 6\n",
    "        self.sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").set(\"spark.driver.memory\", \"15g\"))\n",
    "        self.input = self.sc.textFile(datapath).map(lambda x: (int(x.split(',')[0])-1,\n",
    "                                                               int(x.split(',')[1])-1,int(x.split(',')[2])))\n",
    "        self.A = self.input\n",
    "        self.H = create_uniform_rdd_rowindexed_matrix(self.sc, self.n, self.k, normalize=True)\n",
    "        self.W = create_uniform_rdd_rowindexed_matrix(self.sc, self.m, self.k)\n",
    "        \n",
    "    def compute_H(self):\n",
    "        print(\"Compute H\")\n",
    "    \n",
    "        ## compute X\n",
    "        # M1\n",
    "        W_b = self.sc.broadcast(self.W.collect())\n",
    "        Aw = self.A.map(lambda x: (x[1], x[2]*W_b.value[x[0]][1])) #Wi.T\n",
    "        #print(Aw.collect())\n",
    "        # R1 \n",
    "        X = Aw.reduceByKey(lambda x, y: x+y) # add a combiner? #.map(lambda x: (x[0], x[1]))\n",
    "    \n",
    "        \n",
    "        ## compute B\n",
    "        # M2\n",
    "        ww = self.W.map(lambda x: (None, np.outer(x[1],x[1]))) # w.T.dot(w)? #import numpy.linalg\n",
    "        # R2\n",
    "        B = ww.reduceByKey(lambda x, y: x+y).map(lambda x: x[1]) # without map?\n",
    "        \n",
    "        ## compute Y\n",
    "        # M3\n",
    "        B_b = self.sc.broadcast(B.collect())\n",
    "        y = self.H.map(lambda x: (x[0], B_b.value@x[1]))\n",
    "        \n",
    "        ## update H\n",
    "        # M4\n",
    "        self.hxy = self.H.join(y).join(X)\n",
    "        #print(self.hxj.collect())\n",
    "        # R4\n",
    "        # why is it a reduce phase instead of a map phase?\n",
    "        hnew = self.hxy.map(lambda x: (x[0], update_h(x[1])))\n",
    "        self.H = hnew\n",
    "        \n",
    "    def compute_W(self):\n",
    "        print(\"Compute W\")\n",
    "        #M5: Compute matrix S (repartition join of H and A)\n",
    "        tmp_M5 = self.H.join(self.A.map(lambda x: (x[1], (x[0], x[2]))))\n",
    "\n",
    "        #R5: Emit each hj according to row i\n",
    "        self.tmp_M5 = tmp_M5.map(lambda x: (x[1][1][0], x[1][1][1] * x[1][0]))\n",
    "\n",
    "        \n",
    "        \n",
    "        #M6: Finish the computation of matrix S -- identity mapper so no code needed\n",
    "        #R6: Emit each row in matrix S\n",
    "        self.S = self.tmp_M5.reduceByKey(lambda x, y : x +y)\n",
    "\n",
    "        \n",
    "        #M7: Calculate matrix C (same as calculating B in compute_H)\n",
    "        tmp_M7 = self.H.map(lambda x: (0, np.outer(x[1], x[1])))\n",
    "        #R7: Emit matrix C from the sum of matrices\n",
    "        C = tmp_M7.reduceByKey(lambda x, y : x + y).map(lambda x: x[1])\n",
    "        \n",
    "        #M8: Compute matrix T\n",
    "        C_b = self.sc.broadcast(C.collect())\n",
    "        self.T = self.W.map(lambda x: (x[0], x[1]@C_b.value))\n",
    "        \n",
    "\n",
    "        #M9: update W with broadcast join of S and T. W = W * S/T\n",
    "        T_b = self.sc.broadcast(self.T.collectAsMap())\n",
    "        S_b = self.sc.broadcast(self.S.collectAsMap())\n",
    "        self.W = self.W.map(lambda x: (x[0], x[1]*np.divide(S_b.value[x[0]], T_b.value[x[0]])))\n",
    "        \n",
    "    def iterate_HW(self): # , iteration_time=25, epsilon\n",
    "        i = 0\n",
    "        while i < 5: # or np.norm(previous_H, H) < epsilon\n",
    "            self.compute_H()\n",
    "            self.compute_W()\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "    def assign_clusters(self):\n",
    "        print(\"Assign clusters\")\n",
    "        #M10: Map user to cluster with highest probability\n",
    "        self.clusters = self.H.map(lambda x: (x[0], np.where(x[1][0] == max(x[1][0]))[0][0]))\n",
    "        \n",
    "        #M11: Emit a 1 for each user that is in a cluster\n",
    "        self.clustersizes = self.clusters.map(lambda x: (x[1], 1))\n",
    "        \n",
    "        #R11: Count the number of users per cluster\n",
    "        self.clustersizes = self.clustersizes.reduceByKey(lambda x,y: x+y)\n",
    "        \n",
    "    def RM2_distribution(self):\n",
    "        print(\"Compute RM2_distribution\")\n",
    "        #M12: Emit each rating that a user gave\n",
    "        self.ratings = self.input.map(lambda x: (x[1], x[2]))\n",
    "        \n",
    "        #R12: Sum ratings given by user\n",
    "        self.ratings = self.ratings.reduceByKey(lambda x, y: x+y)\n",
    "        \n",
    "        #Accumulator containing total count of ratings\n",
    "        globalcount = self.sc.accumulator(0)\n",
    "        self.ratings.foreach(lambda x: globalcount.add(x[1]))\n",
    "        \n",
    "        #M13: Emit each rating that an item received\n",
    "        items = self.input.map(lambda x: (x[0], x[2]))\n",
    "        \n",
    "        #Broadcast the total count of ratings, to be used in next reduce\n",
    "        globalcount = self.sc.broadcast(globalcount.value)\n",
    "        \n",
    "        #R13: Compute the probability of item in the collection\n",
    "        items = items.reduceByKey(lambda x,y: x+y)\n",
    "        \n",
    "        #Divide previous values by the total count of ratings, to normalize probabilities\n",
    "        items = items.map(lambda x: (x[0], x[1]/globalcount.value))\n",
    "        \n",
    "        #Broadcast all users and which cluster they are in\n",
    "        clusters = self.sc.broadcast(self.clusters.collectAsMap())\n",
    "        \n",
    "        #M14-a: Map each rating from input to the cluster assigned to user\n",
    "        self.input2cluster = self.input.map(lambda x: (clusters.value[x[1]], (x[1], x[0], x[2]))).groupByKey()\n",
    "        \n",
    "        #M14-b: Map sum of ratings of a user to their assigned cluster\n",
    "        self.ratings2cluster = self.ratings.map(lambda x: (clusters.value[x[0]], (x[0], x[1]))).groupByKey()\n",
    "        \n",
    "        #Repartition join these two maps by cluster\n",
    "        self.fullClusters = self.input2cluster.join(self.ratings2cluster)\n",
    "        \n",
    "        #Broadcast probabilities of each item\n",
    "        items_bc = self.sc.broadcast(items.collectAsMap())\n",
    "        \n",
    "        ld_bc = self.sc.broadcast(self.ld)\n",
    "        \n",
    "        #Final reduce combines all information from above, needs W and H\n",
    "        self.out = self.fullClusters.flatMap(lambda x: (x[0], compute_p_i_R(x[1][0], x[1][1], items_bc.value, ld_bc.value)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d11c71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Algorithm = MRAlgorithm('../data/small_dataset.txt', k=5)\n",
    "#Algorithm = MRAlgorithm('../data/smaller_dataset.txt', k=5, ld=0.5)\n",
    "#Algorithm.iterate_HW()\n",
    "# Algorithm.compute_W()\n",
    "# Algorithm.assign_clusters()\n",
    "# Algorithm.RM2_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b80df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clusters = Algorithm.out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b3dcb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Algorithm.input.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0e5b6",
   "metadata": {},
   "source": [
    " ## Experiments and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae4346",
   "metadata": {},
   "source": [
    " #### evaluation of the recommendation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6690f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data\n",
    "data = pd.read_csv(\"../data/prepared_data_1.txt\", header=None, names=[\"item\", \"user\", \"ranking\"])\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data) # by default 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "798ddf30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>user</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>731970</th>\n",
       "      <td>191</td>\n",
       "      <td>849362</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677405</th>\n",
       "      <td>1902</td>\n",
       "      <td>1041130</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258526</th>\n",
       "      <td>427</td>\n",
       "      <td>1115506</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19287790</th>\n",
       "      <td>3650</td>\n",
       "      <td>2443405</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23596765</th>\n",
       "      <td>4415</td>\n",
       "      <td>1686279</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          item     user  ranking\n",
       "731970     191   849362        4\n",
       "9677405   1902  1041130        2\n",
       "2258526    427  1115506        4\n",
       "19287790  3650  2443405        4\n",
       "23596765  4415  1686279        5"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f7271c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Lu_k(clusters, user_id, k):\n",
    "    # generate a list of k (item, ranking) pairs sorted on ranking for the user ùë¢\n",
    "    Lu = [] # list of all relevant items\n",
    "    # iterate over clusters\n",
    "    for i in range(len(clusters)//2):\n",
    "        relevant_users = [x[1] for x in clusters[2*i+1].keys()]\n",
    "        if user_id in relevant_users:\n",
    "            Lu += [(x[0][0], x[1]) for x in list(clusters[2*i+1].items())]\n",
    "    Lu_unique = list(set(Lu))\n",
    "    Lu_unique.sort(key=lambda x:x[1], reverse=True) \n",
    "    return Lu_unique[:k]\n",
    "\n",
    "def top_k_items(Lu_k):\n",
    "    return [item_ranking_tuple[0] for item_ranking_tuple in Lu_k]\n",
    "    \n",
    "def generate_Ru(df, user_id):\n",
    "    # generate a list of ranking for all the items having a test rating by some user and no training rating by ùë¢\n",
    "    return list(df[df[\"user\"] == user_id][\"item\"])\n",
    "\n",
    "def intersection_proportion(Luk, Ru, k):\n",
    "    return len(set(Luk) & set(Ru)) / k\n",
    "\n",
    "def precision(k, users):\n",
    "    proportions = np.array([intersection_proportion(generate_Lu_k(clusters, i, k), generate_Ru(df, i, k), 10) for i in users])\n",
    "    return np.mean(proportions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1280dec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 356.3258262852286), (0, 145.04464799243766), (5, 66.07494276928142), (4, 37.813576458711005), (3, 11.27704628851279), (1, 2.381702320505256), (2, 1.9098670197560526), (0, 0.7033949162594025), (0, 0.2906373760510468), (4, 0.2316043017684758)]\n",
      "[2, 0, 5, 4, 3, 1, 2, 0, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "Luk = generate_Lu_k(clusters, 766488, 10)\n",
    "res = top_k_items(Luk)\n",
    "print(Luk)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25365827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Ru is generated from the whole dataset while Luk is only generated from the train subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e575a3",
   "metadata": {},
   "source": [
    " #### analysis of the scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088282d2",
   "metadata": {},
   "source": [
    " #### Effectiveness experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22c923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
